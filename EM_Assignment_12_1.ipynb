{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Emaperidol/AI_ML_Healthcare/blob/main/EM_Assignment_12_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Use LightFM to Build a Movie Recommender System"
      ],
      "metadata": {
        "id": "_h3fcABrhIr4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LightFM\n",
        "\n",
        "\\\n",
        "LightFM is a Python implementation of a number of popular recommendation algorithms for both implicit and explicit feedback.\n",
        "\n",
        "\\\n",
        "You should use LightFM to complete this assignment.\n",
        "\n",
        "\\\n",
        "You should read and learn from the following two tutorials regarding how you can bring customized dataset into LightFM and build a recommmender model:\n",
        "\n",
        "* https://making.lyst.com/lightfm/docs/examples/dataset.html\n",
        "* https://making.lyst.com/lightfm/docs/quickstart.html"
      ],
      "metadata": {
        "id": "vD0EBLWBe5e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MovieLens 100k Small Dataset\n",
        "\n",
        "Data Downloading Source: https://www.kaggle.com/datasets/fuzzywizard/movielens-100k-small-dataset?resource=download\n",
        "\n",
        "\\\n",
        "This dataset describes 5-star rating and free-text tagging activity from [MovieLens](http://movielens.org), a movie recommendation service. It contains 100836 ratings and 3683 tag applications across 9742 movies. These data were created by 610 users between March 29, 1996 and September 24, 2018. This dataset was generated on September 26, 2018.\n",
        "\n",
        "\\\n",
        "Users were selected at random for inclusion. All selected users had rated at least 20 movies. No demographic information is included. Each user is represented by an id, and no other information is provided.\n",
        "\n",
        "\\\n",
        "The data are contained in the files `links.csv`, `movies.csv`, `ratings.csv` and `tags.csv`.\n",
        "\n",
        "\\\n",
        "For this homework assignment, you will only use two datasets, namely `movies.csv`, `ratings.csv`.\n"
      ],
      "metadata": {
        "id": "iu41WFsbS-tv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q1.\n",
        "Read the two datasets: \"ratings.csv\" and \"movies.csv\" into two DataFrames, and name them \"ratings_data\" and \"movies_data\" resepctively."
      ],
      "metadata": {
        "id": "y8JZS28Mr0DJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "ratings_data = pd.read_csv('ratings.csv')\n",
        "movies_data = pd.read_csv('movies.csv')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "MoeZnPG_vPNq",
        "outputId": "844b78f5-ec0b-4f78-e88a-06b3005615d3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-9ca8efc3-80ef-492f-8e3a-40fa17e5244f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-9ca8efc3-80ef-492f-8e3a-40fa17e5244f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving movies.csv to movies.csv\n",
            "Saving ratings.csv to ratings.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q2.\n",
        "\n",
        "We’ll use LightFM’s built-in Dataset class to build an interaction dataset from raw data. The goal is to demonstrate how to go from raw data (lists of interactions and perhaps item and user features) to scipy.sparse matrices that can be used to fit a LightFM model.\n",
        "\n",
        "\\\n",
        "First, merge the ratings dataset with movies dataset, and then create unique user ids and movie ids that start from 0.\n",
        "\n",
        "\\\n",
        "Hint: some sample code:\n",
        "```\n",
        "# Merge ratings data with movies data\n",
        "data = ratings_data.merge(movies_data)\n",
        "\n",
        "# Create unique user and movie ids from 0\n",
        "data['user_id'] = data.groupby(['userId']).ngroup()\n",
        "data['movie_id'] = data.groupby(['movieId']).ngroup()\n",
        "\n",
        "# Convert ids from int64 to int32 required by model\n",
        "data['user_id'] = data['user_id'].astype(np.int32)\n",
        "data['movie_id'] = data['movie_id'].astype(np.int32)\n",
        "```"
      ],
      "metadata": {
        "id": "qbpvnjxvkmIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Merging Data\n",
        "data = ratings_data.merge(movies_data, on='movieId')\n",
        "\n",
        "# Unique IDs\n",
        "data['user_id'] = data.groupby(['userId']).ngroup()\n",
        "data['movie_id'] = data.groupby(['movieId']).ngroup()\n",
        "\n",
        "# Converting from int64 to int32\n",
        "data['user_id'] = data['user_id'].astype(np.int32)\n",
        "data['movie_id'] = data['movie_id'].astype(np.int32)\n"
      ],
      "metadata": {
        "id": "XjuuChUSv5cu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q3.\n",
        "Build the ratings dataset in dictionary format.\n",
        "\n",
        "\\\n",
        "Hint: Some sample code:\n",
        "```\n",
        "ratings = data[['userId', 'movieId', 'rating']].to_dict('records')\n",
        "```"
      ],
      "metadata": {
        "id": "RZpe5q97iz99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ratings = data[['userId', 'movieId', 'rating']].to_dict('records')\n"
      ],
      "metadata": {
        "id": "7MUeRQH4wJ3w"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q4.\n",
        "Now let's build the features dataset in dictionary format.\n",
        "\n",
        "\\\n",
        "Hint: The feature dataset contains three variables, namely `movieId`, `title`, and `genres`. Note that the length of the feasture dataset should be the same as the number of unqiue movies."
      ],
      "metadata": {
        "id": "9OttDjawjA2V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = data[['movieId', 'title', 'genres']].drop_duplicates().to_dict('records')"
      ],
      "metadata": {
        "id": "l-k7ddQXwL_T"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q5.\n",
        "\n",
        "Since you have the features dataset and the ratings dataset, now you are ready to build a LightFM Dataset. The LightFM Dataset creates a mapping between the user and item ids from our input data to indices that will be used internally by our model.\n",
        "\n",
        "\\\n",
        "Hint: First create a Dataset and then call its fit method. The first argument is an iterable of all user ids in our data, and the second is an iterable of all item ids. In this case, we use generator expressions to lazily iterate over our data and yield user and item ids. This call will assign an internal numerical id to every user and item id we pass in. These will be contiguous (from 0 to however many users and items we have), and will also determine the dimensions of the resulting LightFM model."
      ],
      "metadata": {
        "id": "UYZt9BNsokSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightfm\n",
        "from lightfm.data import Dataset\n",
        "\n",
        "dataset = Dataset()\n",
        "dataset.fit((x['userId'] for x in ratings),\n",
        "            (x['movieId'] for x in ratings))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRQNwvOvwRix",
        "outputId": "4642f08b-db2b-4db0-9649-bd13d62875e2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightfm in /usr/local/lib/python3.10/dist-packages (1.17)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lightfm) (1.23.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from lightfm) (1.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from lightfm) (2.31.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from lightfm) (1.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->lightfm) (2023.7.22)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lightfm) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->lightfm) (3.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q6.\n",
        "\n",
        "\n",
        "We can check that the mappings have been created by querying the Dataset on how many users and books it knows about.\n",
        "\n",
        "\\\n",
        "Hint: Some sample code:\n",
        "```\n",
        "num_users, num_items = dataset.interactions_shape()\n",
        "print('Num users: {}, num_items {}.'.format(num_users, num_items))\n",
        "```"
      ],
      "metadata": {
        "id": "rGStsluQop1F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_users, num_items = dataset.interactions_shape()\n",
        "print('Num users: {}, num_items {}.'.format(num_users, num_items))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m5Zi1gauwoHA",
        "outputId": "4576c41b-7747-4125-d20b-99d55071de53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num users: 610, num_items 9724.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q7.\n",
        "\n",
        "Having created the mapping, we can build the interaction matrix."
      ],
      "metadata": {
        "id": "s29Mr1aGo6bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(interactions, weights) = dataset.build_interactions(((x['userId'], x['movieId'])\n",
        "                                                      for x in ratings))"
      ],
      "metadata": {
        "id": "3pd_1l_Hwrfu"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q8.\n",
        "\n",
        "Note that if we don’t have all user and items ids at once, we can repeatedly call `fit_partial` to supply additional ids. Now, `fit_partial` to add some item feature mappings. In particular, add movie titles to the item feature mappings."
      ],
      "metadata": {
        "id": "IKFOKXDhp8bb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.fit_partial(items=(x['movieId'] for x in features),\n",
        "                    item_features=(x['title'] for x in features))\n"
      ],
      "metadata": {
        "id": "cqsHL72kwvua"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q9.\n",
        "\n",
        "Since we have item features, we can create the item features matrix."
      ],
      "metadata": {
        "id": "hVIcZS_FpJG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "item_features = dataset.build_item_features(((x['movieId'], [x['title']])\n",
        "                                             for x in features))\n"
      ],
      "metadata": {
        "id": "xEKtsQYnwync"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q10.\n",
        "\n",
        "Randomly split the dataset into train and test set so that the train set contains 80% of the data.\n",
        "\n",
        "\\\n",
        "Hint: Use the `random_train_test_split` function."
      ],
      "metadata": {
        "id": "ED0VtV6WsVJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightfm.cross_validation import random_train_test_split\n",
        "\n",
        "train, test = random_train_test_split(interactions, test_percentage=0.2)\n"
      ],
      "metadata": {
        "id": "oMcVFn_bw0tf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q11.\n",
        "\n",
        "We’re going to use the WARP (Weighted Approximate-Rank Pairwise) model. WARP is an implicit feedback model: all interactions in the training matrix are treated as positive signals, and products that users did not interact with they implicitly do not like. The goal of the model is to score these implicit positives highly while assigining low scores to implicit negatives.\n",
        "\n",
        "\\\n",
        "Model training is accomplished via SGD (stochastic gradient descent). This means that for every pass through the data — an epoch — the model learns to fit the data more and more closely. We’ll run it for 30 epochs in this example."
      ],
      "metadata": {
        "id": "YH4-V_NNr9-I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightfm import LightFM\n",
        "\n",
        "model = LightFM(loss='warp')\n",
        "model.fit(train, item_features=item_features, epochs=30)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bz5kJNbw3_Z",
        "outputId": "366325f2-dee1-4df6-bd55-773c73d30455"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<lightfm.lightfm.LightFM at 0x7af4cf7a9f00>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q12.\n",
        "\n",
        "We should now evaluate the model to see how well it’s doing. We’re most interested in how good the ranking produced by the model is.\n",
        "\n",
        "\\\n",
        "Precision@k is one suitable metric, expressing the percentage of top k items in the ranking the user has actually interacted with.\n",
        "\n",
        "\\\n",
        "Measure Precision@k in both the train and the test set."
      ],
      "metadata": {
        "id": "00HBDft5tSIB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from lightfm.evaluation import precision_at_k\n",
        "\n",
        "print(\"Train precision: %.2f\" % precision_at_k(model, train, item_features=item_features, k=5).mean())\n",
        "print(\"Test precision: %.2f\" % precision_at_k(model, test, item_features=item_features, k=5).mean())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vAVDwWGw8ge",
        "outputId": "321ba8b5-5f77-4c51-df73-1888e09bd40b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train precision: 0.46\n",
            "Test precision: 0.11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Q13.\n",
        "\n",
        "Unsurprisingly, the model fits the train set better than the test set.\n",
        "\n",
        "\\\n",
        "For an alternative way of judging the model, we can sample a couple of users and get their recommendations.\n",
        "\n",
        "\\\n",
        "To make predictions for given user, we pass the id of that user and the ids of all products we want predictions for into the predict method.\n",
        "\n",
        "Now, please write a function that can show: (1) five of the movies that a particular user had rated; and (2) the top five movies the recommender model recommend.\n",
        "\n",
        "For example, your function can show the following:\n",
        "```\n",
        "User ID:  12\n",
        "\n",
        "Five of the movies the user has watched:\n",
        "                         Toy Story (1995)\n",
        "                         Tommy Boy (1995)\n",
        "          Clear and Present Danger (1994)\n",
        "                          Saturn 3 (1980)\n",
        "Twelve Monkeys (a.k.a. 12 Monkeys) (1995)\n",
        "\n",
        "\n",
        "Top five movies the model recommends:\n",
        "            Crimson Tide (1995)\n",
        "              Disclosure (1994)\n",
        "  Star Trek: Generations (1994)\n",
        "Clear and Present Danger (1994)\n",
        "               Quiz Show (1994)\n",
        "```"
      ],
      "metadata": {
        "id": "8Aa-TFf0ttVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def user_recommendations(user_id, model, data, item_features):\n",
        "    # Movies watched by user\n",
        "    known_positives = data[data['userId'] == user_id]['title'].head(5).tolist()\n",
        "\n",
        "    # Movies predicted to be liked by user\n",
        "    scores = model.predict(user_id, np.arange(num_items), item_features=item_features)\n",
        "    top_items = data['title'][np.argsort(-scores)].head(5)\n",
        "\n",
        "    return user_id, known_positives, top_items.tolist()\n",
        "\n",
        "user_id = 13\n",
        "user_id, known_positives, top_items = user_recommendations(user_id, model, data, item_features)\n",
        "print(f\"User ID:  {user_id}\\n\")\n",
        "print(f\"Five of the movies the user has watched: \\n{known_positives}\\n\")\n",
        "print(f\"Top five movies the model recommends: \\n{top_items}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qeNsG_I8xcTP",
        "outputId": "54f850fd-3215-413a-b50d-d1c9b02d08df"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "User ID:  13\n",
            "\n",
            "Five of the movies the user has watched: \n",
            "['Seven (a.k.a. Se7en) (1995)', 'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)', 'Beetlejuice (1988)', 'Matrix, The (1999)', 'Gladiator (2000)']\n",
            "\n",
            "Top five movies the model recommends: \n",
            "['Heat (1995)', 'Usual Suspects, The (1995)', 'Usual Suspects, The (1995)', 'Seven (a.k.a. Se7en) (1995)', 'Seven (a.k.a. Se7en) (1995)']\n",
            "\n"
          ]
        }
      ]
    }
  ]
}